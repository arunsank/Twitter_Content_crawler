{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Twitter Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Prerequisite:\n",
    "\n",
    "Creating a Twitter application using the Twitter API\n",
    "\n",
    "Approach:\n",
    "\n",
    "For our approach I have created Spark parquet files by extracting tweets in real time from Twitter. \n",
    "The below code \n",
    "1. extracts tweets , location and user handle. (for our apprach the #jobsearch is used as the criterion, other filters can be changed based on requirement) \n",
    "2. performs sentimental analysis using the Vader Algorithm.\n",
    "3. Create Parquet files which is highly effecient and quick in data extraction using SparkSQL\n",
    "4. Export data as CSV for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import googlemaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pyspark.sql import SparkSession\n",
    "from geopy.geocoders import Nominatim\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from twitter import Twitter, OAuth, TwitterHTTPError, TwitterStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Sentiment analyzer\n",
    "sa = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve(): \n",
    "        #Return data from Twitter using twitter application credentials\n",
    "        ACCESS_TOKEN = ''\n",
    "        ACCESS_SECRET = ''\n",
    "        CONSUMER_KEY = ''\n",
    "        CONSUMER_SECRET = ''\n",
    "        #Read train data with sentiment positive and negative\n",
    "        oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n",
    "        twitter_stream = TwitterStream(auth=oauth)\n",
    "        twitter = Twitter(auth=oauth)\n",
    "        x = []\n",
    "        for i in range(0,1):\n",
    "            #Read tweets with #jobsearch\n",
    "            iterator = twitter_stream.statuses.filter(track=\"#jobsearch\", language = \"en\")\n",
    "            x.append(iterator)\n",
    "            \n",
    "        #classify tweets based on sentiments    \n",
    "        df= Classify_tweet(x)\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Classify_tweet(x):\n",
    "        \n",
    "        collectobj = []\n",
    "        for iterator in x:\n",
    "            for tweet in iterator:\n",
    "                if 'user' in tweet.keys() and tweet['user']['location']:\n",
    "                    #Get probability of a tweet being positive, negative or neutral\n",
    "                    topic = getprobtop(tweet['text'])\n",
    "                    obj = {'id' : tweet['user']['id'], 'User_name' : tweet['user']['screen_name'], \\\n",
    "                'Text': tweet['text'],'location' : tweet['user']['location'], \\\n",
    "                'status' : topic}\n",
    "                    print(\"Returning from prob\")\n",
    "                    collectobj.append(obj)\n",
    "\n",
    "        table = json_normalize(collectobj)\n",
    "        return(table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getprobtop(text):\n",
    "        #use sentiment analyzer to get scores based on sentiments\n",
    "        text = text.encode('utf8')\n",
    "        text = text.decode().split('@')\n",
    "\n",
    "        text = [x for x in text if '#jobsearch' in x.lower()]\n",
    "        text = ' '.join(text)\n",
    "        text  = ' '.join([x for x in text.split(' ') if '#jobsearch' not in x.lower()])\n",
    "        text  = ' '.join([x for x in text.split(' ') if '#' not in x.lower()])\n",
    "        t = re.match('(.*?)http.*?\\s?(.*?)', text)\n",
    "        if t:\n",
    "            text = t.group(1)\n",
    "        score = sa.polarity_scores(text.lower())\n",
    "        score = {'neg' : score['neg'],'pos' : score['pos'],'neu' : score['neu']}\n",
    "        key = max(score,key=score.get)\n",
    "        value = score[key]\n",
    "        if key == 'neu' and (score['pos']<=0.0 and score['neg']<=0.0):\n",
    "            key = key;value = value\n",
    "        else:\n",
    "            del score['neu']\n",
    "            key = max(score,key=score.get)\n",
    "            value = score[key]\n",
    "        print (text)\n",
    "        print (key,value)\n",
    "        return key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "        #Sparksession builder\n",
    "        spark = SparkSession.builder \\\n",
    "                 .master(\"local\") \\\n",
    "                 .appName(\"Word Count\") \\\n",
    "                 .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "                 .getOrCreate()\n",
    "        \n",
    "        for i in range(1,2):\n",
    "            print(i)\n",
    "            q = retrieve()\n",
    "            spark.createDataFrame(q).write.parquet(\"infile.parquet\") #Outputs the data to local FS as a parquet file\n",
    "            df = spark.read.parquet(\"infile.parquet\")\n",
    "            df.write.csv(\"outfile.csv\")\n",
    "            time.sleep(15*15)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Start of extraction\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
